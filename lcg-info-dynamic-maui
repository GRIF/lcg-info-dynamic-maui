#!/usr/bin/python
"""
GIP plugin updating CE and cluster attributes based on MAUI information.

This plugin is an alternative to the old PBS-based GIP plugin. It takes into accounts
specific MAUI features like standing reservations.
"""

__version__ = "2.1.0-3"
__author__  = "Michel Jouvin <jouvin@lal.in2p3.fr>, Cedric Duprilot <duprilot@lal.in2p3.fr>"


import sys
import logging
import syslog
import getopt
import string
import os
import re
from optparse import OptionParser
try:
  from TorqueMauiConfParser import *
except:
  sys.stderr.write("Failed to load TorqueMauiConfParser module. Check PYTHONPATH is appropriate.")
  sys.exit(1)


# Initializations
verbosity = 0
# Allow to define the max number of waiting job from the max number of running jobs allowed.
# Can be modified by option --waiting-running-ratio.
max_waiting_running_def_ratio = 2


# convert triplet hours:minutes:seconds into seconds
def convertHhMmSs(timeStr):
  diagPattern = re.compile(r'(?P<hour>\d+):(?P<minute>\d+):(?P<second>\d+)')
  matcher = diagPattern.search(timeStr)
  if matcher:
    hours = int(matcher.group('hour'))
    minutes = int(matcher.group('minute'))
    secondes = int(matcher.group('second'))
    time = hours * 3600 + minutes * 60 + secondes
  elif timeStr != "-":
    time = 0
  return int(time)

# note: this class works around the bug in the python logging
# package fixed in patch #642974.  If the version of python-logging
# is upgraded, this class can be replaced by logging.SysLogHandler
class SLHandler(logging.Handler):
    def __init__(self, ident, logopt=0, facility=syslog.LOG_USER):
        logging.Handler.__init__(self)
        self.ident = ident
        self.logopt = logopt
        self.facility = facility
        self.mappings = {
            logging.DEBUG: syslog.LOG_DEBUG,
            logging.INFO: syslog.LOG_INFO,
            logging.WARN: syslog.LOG_WARNING,
            logging.ERROR: syslog.LOG_ERR,
            logging.CRITICAL: syslog.LOG_CRIT,
            }
                                                                                                        
    def encodeLevel(self, level):
        return self.mappings.get(level, syslog.LOG_INFO)
                                                                                                        
    def emit(self, record):
        syslog.openlog(self.ident, self.logopt, self.facility)
        msg = self.format(record)
        prio = self.encodeLevel(record.levelno)
        syslog.syslog(prio, msg)
        syslog.closelog()
                                                                                                        

def abort_without_output(msg):
    logging.error(msg)
    logging.error("Exiting without output, GIP will use static values")
    sys.exit(2)

def debug(level,msg):
  if level <= verbosity:
    sys.stderr.write("%s\n" % msg)


# Initialize logger
logging.getLogger("").setLevel(logging.INFO)
# syslog handler
shdlr = SLHandler("lcg-info-dynamic-maui")
logging.getLogger("").addHandler(shdlr)
# stderr handler
stdrhdlr = logging.StreamHandler()
fmt=logging.Formatter("%(asctime)s lcg-info-dynamic-maui:"
                      + " %(message)s","%F %T")
logging.getLogger("").addHandler(stdrhdlr)
stdrhdlr.setFormatter(fmt)

# Initialize parser
parser = OptionParser()
parser.add_option('--ce-ldif', dest='ce_ldif_file', action='store', default=None, help="LDIF file describing GLUE1 CE resources to update")
parser.add_option('--cluster-ldif', dest='cluster_ldif_file', default=None, action='store', help="LDIF file describing GLUE1 cluster resources to update (DEPRECATED)")
parser.add_option('-H','--host','--server', dest='hostname', default=None, action='store', help="Host name of Torque/MAUI server")
parser.add_option('-v', '--debug', dest='verbosity', action='count', default=0, help='Increase verbosity level for debugging (on stderr)')
parser.add_option('--diagnose-output', dest='diag_output', action='store', default=None, help='File containing a diagnose -r output. Be sure it is up-to-date.')
# Flavor is kept for backward compatibility but is ignored
parser.add_option('--flavor', dest='ce_flavor', default='all', action='store', help="CE flavor (cream or all)")
parser.add_option('--max-normal-slots', dest='max_normal_slots', action='store', type="int", default=None, help="Maximum number of normal slots")
parser.add_option('--share-ldif', dest='share_ldif_file', action='store', default=None, help="LDIF file describing GLUE2 share resources to update")
parser.add_option('--version', dest='version', action='store_true', default=False, help='Display various information about this script')
parser.add_option('--waiting-running-ratio', dest='max_waiting_running_ratio', action='store', type="int", default=max_waiting_running_def_ratio, \
                                             help="Ratio between max number of waiting jobs and max number of running jobs (D: %d)" % (max_waiting_running_def_ratio))
options, args = parser.parse_args()

if options.verbosity:
  verbosity = options.verbosity
  
if options.version:
  debug (0,"Version %s written by %s" % (__version__,__author__))
  debug (0,__doc__)
  sys.exit(0)

if not options.ce_ldif_file and not options.share_ldif_file and not options.cluster_ldif_file:
    parser.print_usage()
    abort_without_output("At least one of the options --ce-ldif, --share-ldif or --cluster-ldif must be specified.")
    
    
if options.hostname:
    schedhost = options.hostname
else:
    schedhost = os.popen('hostname -f').readline().strip('\n')
    debug (1,"No scheduler host specified. Using default (%s)" % (schedhost))

if options.ce_flavor and not re.match('all|cream',options.ce_flavor):
  abort_without_output("Invalid value for deprecated --flavor option (%s): must be 'all' or 'cream'." % (options.ce_flavor))

# Initializations

ce_dn_to_queue = {}               # GLUE1
share_dn_to_queue = {}            # GLUE2
subcluster_dn_list = []
output = []
queue_attrs= {}

# Retrieve GLUE1 CE DNs and GLUEE2 share DNs from static LDIF files
# Assume that both files are equivalent and can contain both GLUE1 and GLUE2 DNs.

ldif_files = []
if options.ce_ldif_file:
  ldif_files.append(options.ce_ldif_file)
if options.share_ldif_file:
  ldif_files.append(options.share_ldif_file)
  
if len(ldif_files) == 0:
  debug (1,'No CE or share static LDIF file specified. Ignoring update of CE parameters')

# found_xxx_num are for debugging purpose
found_ce_num = 0
found_share_num = 0

for file in ldif_files:
  try:
    static_ldif = open(file,'r')
  except IOError:
    abort_without_output("Error opening static LDIF file %s" % (file))
      
  jobmanager_prefix = 'cream'
  jobmanager_name = 'pbs'
  ceIDPattern = re.compile(r'^dn:\s+GlueCEUniqueID=[\w\-\.:]+/+(?:'+jobmanager_prefix+')-(?:'+jobmanager_name+')-(?P<queue>[\w\-\.]+),')
  shareIDPattern = re.compile(r'^dn:\s+GLUE2ShareID=.*GLUE2GroupID=resource,o=glue')
  shareQueuePattern = re.compile(r'^GLUE2ComputingShareMappingQueue:\s+(?P<queue>[\w\-\.]+)')
  share_dn = None
  
  for line in static_ldif:
    if share_dn:
      # Look for line defining the associated queue
      if line == '\n':
        logging.error("GLUE2ComputingShareMappingQueue not found for share %s" % (share_dn))        
      matcher_g2 = shareQueuePattern.match(line)
      if matcher_g2:
        share_dn_to_queue[share_dn] = matcher_g2.group('queue')
        # Add queue to queue list, parameters will be retrieved later
        queue_attrs[matcher_g2.group('queue')] = None
        share_dn = None         
    else:
      matcher_g2 = shareIDPattern.match(line)
      if matcher_g2:
        share_dn = line.strip('\n')
      else:
        matcher_g1 = ceIDPattern.match(line)
        if matcher_g1:
          ce_dn_to_queue[line.strip('\n')] = matcher_g1.group('queue')
          # Add queue to queue list, parameters will be retrieved later
          queue_attrs[matcher_g1.group('queue')] = None         
  static_ldif.close()
  debug (1,'Number of GLUE1 CE DNs found in %s: %d' %(file,len(ce_dn_to_queue)-found_ce_num))
  debug (1,'Number of GLUE2 share DNs found in %s: %d' %(file,len(share_dn_to_queue)-found_share_num))
  found_ce_num = len(ce_dn_to_queue)
  found_share_num = len(share_dn_to_queue)
debug (1,'Total number of GLUE1 CE DNs found: %d' %(found_ce_num))
debug (1,'Total number of GLUE2 share DNs found: %d' %(found_share_num))


# Retrieving subcluster DNs from static LDIF file
    
if options.cluster_ldif_file:
  try:
    subcluster_ldif = open(options.cluster_ldif_file,'r')
  except IOError:
    abort_without_output("Error opening config file " + options.cluster_ldif_file)

  # gets the DNs containing the keyword GlueSubClusterUniqueID
  # and put this DN in subcluster_dn_list list
  subclusterIDPattern = re.compile(r'dn:\s+GlueSubClusterUniqueID=')
  for line in subcluster_ldif:
    if subclusterIDPattern.search(line):
      subcluster_dn_list.append(line.strip('\n'))
  subcluster_ldif.close()

else:
  debug (1,'No cluster static LDIF file specified. Ignoring update of subcluster parameters')


# Initialize MAUI and PBS configuration parsers
torqueMauiConf = TorqueMauiConfParser(schedhost,verbosity=verbosity,diagOutputFile=options.diag_output)

# From Torque, get Torque version, Total and Free CPUs
torqueParams = {};
torqueParams['torqueVersion'] = torqueMauiConf.getTorqueVersion()
torqueParams['totalJobSlots'] = torqueMauiConf.getProcNum(activeOnly=True)

#Get number of SDJ job slots configured, used (globally)
SDJSlotsStats = torqueMauiConf.getQueueSDJSlots()
torqueParams['totalSDJSlots'] = SDJSlotsStats['total']
torqueParams['usedSDJSlots'] = SDJSlotsStats['used']

#Get total number of used job slots (properly counting MPI jobs)
torqueParams['totalUsedSlots'] = torqueMauiConf.getTotalUsedSlots()

# Compute the total number of slots for non-SDJ jobs and the amount used
torqueParams['normalJobSlots'] = torqueParams['totalJobSlots'] - torqueParams['totalSDJSlots'];
if torqueParams['normalJobSlots'] < 0:
  debug(1,'Number of slots for normal jobs (%d) negative. Reset to 0.' % torqueParams['normalJobSlots'])
  torqueParams['normalJobSlots'] = 0
elif options.max_normal_slots and torqueParams['normalJobSlots'] > options.max_normal_slots:
  debug(1,'Number of slots for normal jobs (%d) greater than the maximum number defined. Resetting to %d.' % (torqueParams['normalJobSlots'],options.max_normal_slots))
  torqueParams['normalJobSlots'] = options.max_normal_slots
  
torqueParams['freeNormalSlots'] = torqueParams['normalJobSlots'] - (torqueParams['totalUsedSlots'] - torqueParams['usedSDJSlots'])
if torqueParams['freeNormalSlots'] < 0:
  debug(1,'Number of free slots for normal jobs negative (%d). Reset to 0.' % torqueParams['freeNormalSlots'])
  torqueParams['freeNormalSlots'] = 0
elif torqueParams['freeNormalSlots'] > torqueParams['normalJobSlots']:
  debug(1,'Free slots for normal jobs (%d) > total slot number. Reset to %d.' % (torqueParams['freeNormalSlots'],torqueParams['normalJobSlots']))
  torqueParams['freeNormalSlots'] = torqueParams['normalJobSlots']
torqueParams['usedNormalSlots'] = torqueParams['normalJobSlots'] - torqueParams['freeNormalSlots']

# Compute the total number of actually available job slots.
# Will be the maximum number of available slots for any queue.
torqueParams['totalFreeSlots'] = torqueParams['totalJobSlots'] - torqueParams['totalUsedSlots']
if torqueParams['totalFreeSlots'] < 0:
  debug(1,'Total number of available slots negative (%d). Reset to 0.' % torqueParams['totalFreeSlots'])
  torqueParams['totalFreeSlots'] = 0

debug(1,'Number of job slots on active nodes: normal=%d, SDJ=%d' %(torqueParams['normalJobSlots'],torqueParams['totalSDJSlots']))
debug(1,'Number of used job slots on active nodes: %d (normal:%d, SDJ:%d)' %(torqueParams['totalUsedSlots'],torqueParams['usedNormalSlots'],torqueParams['usedSDJSlots']))
debug(1,'Number of available job slots (max for any queue with SDJ enabled): %d' %(torqueParams['totalFreeSlots']))


# Build Glue information for each queue having an entry in the CE static LDIF file.
# DN associated with a queue is in ce_dn_to_queue, retrieved for CE static LDIF file.

if options.ce_ldif_file:
  # First retrieve queue attributes for each queue exposed through a CE
  for queue in queue_attrs:
    # Retrieve queue params. If the returned value is None, it means
    # the queue doesn't exist. In this case stop processing here for this queue.
    queue_attrs[queue] = torqueMauiConf.getQueueParams(queue)
    if queue_attrs[queue] is None:
      logging.error("Queue '%s' doesn't exist" % (queue))
      debug(1,"Queue '%s' doesn't exist" % (queue))
      continue

    # Compute total/free CPUs (slots)
    queue_attrs[queue]['SDJSlots'] = torqueMauiConf.getQueueSDJSlots(queue)    
    queue_attrs[queue]['TotalSlots'] = torqueParams['normalJobSlots'] + queue_attrs[queue]['SDJSlots']['total']
    queue_attrs[queue]['freeJobSlots'] = torqueParams['freeNormalSlots'] + queue_attrs[queue]['SDJSlots']['free']
    if queue_attrs[queue]['freeJobSlots'] < 0:
      debug(1,'Number of free job slots negative (%d). Reset to 0.' % (queue_attrs[queue]['freeJobSlots']))
      queue_attrs[queue]['freeJobSlots'] = 0
    elif queue_attrs[queue]['freeJobSlots'] > torqueParams['totalFreeSlots']:
      debug(1,'Number of free slots in queue (%d) > total number of free slots. Reset to %d.' % (queue_attrs[queue]['freeJobSlots'],torqueParams['totalFreeSlots']))
      queue_attrs[queue]['freeJobSlots'] = torqueParams['totalFreeSlots']
      
    # Get/normalize the value of the following attributes:
    # GlueCEPolicyMaxCPUTime, GlueCEPolicyMaxTotalJobs, GlueCEPolicyPriority, 
    # GlueCEPolicyMaxRunningJobs, GlueCEPolicyMaxWallClockTime, GlueCEStateStatus

    queueEnabled = False
    queueStarted = False
    maxCPUTime = 0
    maxPCPUTime = 0
    
    # For MaxCPUTime, use Torque cput if pcput (per process limit) is not set or > cput.
    # Else use pcput (real limit for single process jobs, not very meaningful anyway for MPI jobs).
    # If none are defined, keep the default value (don't redefine here).
    if 'cput' in queue_attrs[queue]['resources_max']:
    	maxCPUTime = convertHhMmSs(queue_attrs[queue]['resources_max']['cput'][0])
    if 'pcput' in queue_attrs[queue]['resources_max']:
    	maxPCPUTime = convertHhMmSs(queue_attrs[queue]['resources_max']['pcput'][0])
    if (maxCPUTime > 0) and ((maxPCPUTime == 0) or (maxCPUTime < maxPCPUTime)):
      queue_attrs[queue]['MaxCPUTime'] = maxCPUTime
    elif maxPCPUTime > 0:
      queue_attrs[queue]['MaxCPUTime'] = maxPCPUTime

    # Set GlueCEStateStatus according to 'enabled' and 'started'
    if 'enabled' in queue_attrs[queue] and (queue_attrs[queue]['enabled'][0].lower() == 'true'):
    	queueEnabled = True
    else:
    	queueEnabled = False
    if 'started' in queue_attrs[queue] and (queue_attrs[queue]['started'][0].lower() == 'true'):
    	queueStarted = True
    else:
    	queueStarted = False
    if queueEnabled and queueStarted:
      queue_attrs[queue]['Status'] = 'Production'
    elif queueEnabled:
      queue_attrs[queue]['Status'] = 'Queueing'
    elif queueStarted:
      queue_attrs[queue]['Status'] = 'Draining'
    else:
      queue_attrs[queue]['Status'] = 'Closed'

    # Compute max running jobs, max waiting jobs and max total jobs based on queue configuration
    if 'max_running' in queue_attrs[queue]:
      queue_attrs[queue]['MaxRunningJobs'] = queue_attrs[queue]['max_running'][0]
    else:
      queue_attrs[queue]['MaxRunningJobs'] = queue_attrs[queue]['TotalSlots']
    if 'max_queuable' in queue_attrs[queue]:
      queue_attrs[queue]['MaxWaitingJobs'] = queue_attrs[queue]['max_queuable'][0] - queue_attrs[queue]['MaxRunningJobs']
      if queue_attrs[queue]['MaxWaitingJobs'] < 0:
        debug(1,"Queue %s: negative MaxWaitingJobs (%d) reset to 0" % (queue,queue_attrs[queue]['MaxWaitingJobs']))
        queue_attrs[queue]['MaxWaitingJobs'] = 0
      queue_attrs[queue]['MaxTotalJobs'] = queue_attrs[queue]['max_queuable'][0]
    else:
      queue_attrs[queue]['MaxWaitingJobs'] = options.max_waiting_running_ratio * queue_attrs[queue]['MaxRunningJobs']
      queue_attrs[queue]['MaxTotalJobs'] = queue_attrs[queue]['MaxRunningJobs'] + queue_attrs[queue]['MaxWaitingJobs']


  # GLUE1 output
  for dn in ce_dn_to_queue:
    queue = ce_dn_to_queue[dn]
    if queue_attrs[queue] is None:
      logging.error("Skipping GLUE1 GlueCE %s: matching queue (%s) attributes not retrieved" % (dn,queue))
      debug(1,"Skipping GLUE1 GlueCE %s: matching queue (%s) attributes not retrieved" % (dn,queue))
      continue
    output.append(dn)
    output.append('GlueCEInfoLRMSVersion: '+torqueParams['torqueVersion'])
    # For historical reasons, there are 2 attributes representing total number of CPUs
    output.append('GlueCEInfoTotalCPUs: '+str(queue_attrs[queue]['TotalSlots']))
    output.append('GlueCEPolicyAssignedJobSlots: '+str(queue_attrs[queue]['TotalSlots']))
    # For historical reasons, there are 2 attributes representing number of free slots
    output.append('GlueCEStateFreeCPUs: '+str(queue_attrs[queue]['freeJobSlots']))
    output.append('GlueCEStateFreeJobSlots: '+str(queue_attrs[queue]['freeJobSlots']))
    if 'max_queuable' in queue_attrs[queue]:
      output.append('GlueCEPolicyMaxTotalJobs: %s' % (queue_attrs[queue]['max_queuable'][0]))
    if 'Priority' in queue_attrs[queue]:
      output.append('GlueCEPolicyPriority: %s' % (queue_attrs[queue]['Priority'][0]))
    if 'max_running' in queue_attrs[queue]:
      output.append('GlueCEPolicyMaxRunningJobs: %s' % (queue_attrs[queue]['max_running'][0]))
    else:
      output.append('GlueCEPolicyMaxRunningJobs: %s' % torqueParams['normalJobSlots'])
    if 'walltime' in queue_attrs[queue]['resources_max']:
      output.append('GlueCEPolicyMaxWallClockTime: %d' % (convertHhMmSs(queue_attrs[queue]['resources_max']['walltime'][0])/60))
    if 'MaxCPUTime' in queue_attrs[queue]:
      output.append('GlueCEPolicyMaxCPUTime: %d' % (queue_attrs[queue]['MaxCPUTime']/60))
    output.append('GlueCEStateStatus: %s' % (queue_attrs[queue]['Status']))
    # End DN attributes by an empty line
    output.append('\n')

  # GLUE2 output
  # FIXME: properly define default values (instead of duplicating max values)
  # FIXME: define GLUE2ComputingShareMaxMainMemory
  # FIXME: define GLUE2ComputingShareMaxSlotsPerJob
  # FIXME: define GLUE2ComputingShareMaxVirtualMemory
  # FIXME: define GLUE2ComputingShareUsedSlots
  for dn in share_dn_to_queue:
    queue = share_dn_to_queue[dn]
    if queue_attrs[queue] is None:
      logging.error("Skipping GLUE2 share %s: matching queue (%s) attributes not retrieved" % (dn,queue))
      debug(1,"Skipping GLUE2 share %s: matching queue (%s) attributes not retrieved" % (dn,queue))
      continue
    output.append(dn)
    output.append('GLUE2ComputingShareFreeSlots: '+str(queue_attrs[queue]['freeJobSlots']))
    if 'walltime' in queue_attrs[queue]['resources_max']:
      output.append('GLUE2ComputingShareMaxWallTime: %d' % (convertHhMmSs(queue_attrs[queue]['resources_max']['walltime'][0])/60))
      output.append('GLUE2ComputingShareDefaultWallTime: %d' % (convertHhMmSs(queue_attrs[queue]['resources_max']['walltime'][0])/60))
    if 'MaxCPUTime' in queue_attrs[queue]:
      output.append('GLUE2ComputingShareMaxCPUTime: %d' % (queue_attrs[queue]['MaxCPUTime']/60))
      output.append('GLUE2ComputingShareDefaultCPUTime: %d' % (queue_attrs[queue]['MaxCPUTime']/60))
    output.append('GLUE2ComputingShareMaxRunningJobs: %d' % (queue_attrs[queue]['MaxRunningJobs']))
    output.append('GLUE2ComputingShareMaxWaitingJobs: %d' % (queue_attrs[queue]['MaxWaitingJobs']))
    output.append('GLUE2ComputingShareMaxTotalJobs: %d' % (queue_attrs[queue]['MaxTotalJobs']))
    output.append('GLUE2ComputingShareServingState: %s' % (queue_attrs[queue]['Status']))
    # End DN attributes by an empty line
    output.append('\n')


# Update SubCluster logical CPUs if cluster_ldif_file has been specified.
# REPRECATED: this part is maintained for backward compatibility but is not compliant
# with the agreement that GlueSubClusterLogicalCPUs is a static value representing
# the number of cores configured (and not active) in the cluster.
# It is also not able to cope with multiple homogeneous subclusters in a cluster.

if options.cluster_ldif_file:
  for dn in subcluster_dn_list:
    output.append(dn)
    output.append('GlueSubClusterLogicalCPUs: '+str(torqueParams['normalJobSlots']))
    output.append('\n')


# Print results
    
for line in output:
  print line

